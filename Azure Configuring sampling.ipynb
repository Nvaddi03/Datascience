{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a8630e",
   "metadata": {},
   "source": [
    "### Azure Configuring sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dfb4e",
   "metadata": {},
   "source": [
    "#### Grid sampling\n",
    "\n",
    "Grid sampling can only be employed when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space.\n",
    "\n",
    "For example, in the following code example, grid sampling is used to try every possible combination of discrete batch_size and learning_rate value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "              }\n",
    "\n",
    "param_sampling = GridParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4a26d",
   "metadata": {},
   "source": [
    "#### Random sampling\n",
    "\n",
    "Random sampling is used to randomly select a value for each hyperparameter, which can be a mix of discrete and continuous values as shown in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "\n",
    "param_sampling = RandomParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79131d4c",
   "metadata": {},
   "source": [
    "#### Bayesian sampling\n",
    "\n",
    "Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection. The following code example shows how to configure Bayesian sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ed981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': uniform(0.05, 0.1)\n",
    "              }\n",
    "\n",
    "param_sampling = BayesianParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10351ed8",
   "metadata": {},
   "source": [
    "You can only use Bayesian sampling with choice, uniform, and quniform parameter expressions, and you can't combine it with an early-termination policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee35b66",
   "metadata": {},
   "source": [
    "### Running a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9230c6e",
   "metadata": {},
   "source": [
    "In Azure Machine Learning, you can tune hyperparameters by running a hyperdrive experiment.\n",
    "\n",
    "#### Creating a training script for hyperparameter tuning\n",
    "\n",
    "To run a hyperdrive experiment, you need to create a training script just the way you would do for any o ther training experiment, except that your script must:\n",
    "\n",
    "Include an argument for each hyperparameter you want to vary.\n",
    "Log the target performance metric. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.\n",
    "For example, the following example script trains a logistic regression model using a --regularization argument to set the regularization rate hyperparameter, and logs the accuracy metric with the name Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the training dataset\n",
    "data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels, and split for training/validatiom\n",
    "X = data[['feature1','feature2','feature3','feature4']].values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model with the reg hyperparameter\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate and log accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773334d0",
   "metadata": {},
   "source": [
    "##### Note\n",
    "Note that in the Scikit-Learn LogisticRegression class, C is the inverse of the regularization rate; hence C=1/reg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d457",
   "metadata": {},
   "source": [
    "#### Configuring and running a hyperdrive experiment\n",
    "\n",
    "To prepare the hyperdrive experiment, you must use a HyperDriveConfig object to configure the experiment run, as shown in the following example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "# Assumes ws, script_config and param_sampling are already defined\n",
    "\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config,\n",
    "                              hyperparameter_sampling=param_sampling,\n",
    "                              policy=None,\n",
    "                              primary_metric_name='Accuracy',\n",
    "                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                              max_total_runs=6,\n",
    "                              max_concurrent_runs=4)\n",
    "\n",
    "experiment = Experiment(workspace = ws, name = 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9cf8e",
   "metadata": {},
   "source": [
    "#### Monitoring and reviewing hyperdrive runs\n",
    "\n",
    "You can monitor hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks RunDetails widget.\n",
    "\n",
    "The experiment will initiate a child run for each hyperparameter combination to be tried, and you can retrieve the logged metrics these runs using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child_run in run.get_children():\n",
    "    print(child_run.id, child_run.get_metrics())\n",
    "\n",
    "# You can also list all runs in descending order of performance like this:\n",
    "for child_run in hyperdrive_run.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "\n",
    "#To retrieve the best performing run, you can use the following code:\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90edd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1fc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4658317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
