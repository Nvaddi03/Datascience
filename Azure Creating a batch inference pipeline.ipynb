{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0a6296",
   "metadata": {},
   "source": [
    "### Azure Creating a batch inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2183d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Register a model\n",
    "\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=your_workspace,\n",
    "                                      model_name='classification_model',\n",
    "                                      model_path='model.pkl', # local path\n",
    "                                      description='A classification model')\n",
    "\n",
    "#using Run\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49711e3",
   "metadata": {},
   "source": [
    "#### 2. Create a scoring script\n",
    "\n",
    "Batch inferencing service requires a scoring script to load the model and use it to predict new values. It must include two functions:\n",
    "\n",
    "init(): Called when the pipeline is initialized.\n",
    "run(mini_batch): Called for each batch of data to be processed.\n",
    "Typically, you use the init function to load the model from the model registry, and use the run function to generate predictions from each batch of data and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3487f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Create a scoring script\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for model input\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73277dcc",
   "metadata": {},
   "source": [
    "#### 3. Create a pipeline with a ParallelRunStep\n",
    "\n",
    "Azure Machine Learning provides a type of pipeline step specifically for performing parallel batch inferencing. Using the ParallelRunStep class, you can read batches of files from a File dataset and write the processing output to a OutputFileDatasetConfig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335da0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets['batch-data']\n",
    "\n",
    "# Set the output location\n",
    "default_ds = ws.get_default_datastore()\n",
    "output_dir = OutputFileDatasetConfig(name='inferences')\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='batch_scripts',\n",
    "    entry_script=\"batch_scoring_script.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=aml_cluster,\n",
    "    node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a34037",
   "metadata": {},
   "source": [
    "#### 4. Run the pipeline and retrieve the step output\n",
    "\n",
    "After your pipeline has been defined, you can run it and wait for it to complete. Then you can retrieve the parallel_run_step.txt file from the output of the step to view the results, as shown in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e464807",
   "metadata": {},
   "source": [
    "### Publishing a batch inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8695755",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    "                                                   description='Batch pipeline',\n",
    "                                                   version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9ff6c",
   "metadata": {},
   "source": [
    "Once published, you can use the service endpoint to initiate a batch inferencing job, as shown in the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6e6eb",
   "metadata": {},
   "source": [
    "You can also schedule the published pipeline to have it run automatically, as shown in the following example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fddf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                                        description='batch inferencing',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Batch_Prediction',\n",
    "                                        recurrence=weekly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
