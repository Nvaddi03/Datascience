{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3aa9e6a",
   "metadata": {},
   "source": [
    "### Azure Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087682eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OutputFileDatasetConfig Step Inputs and Outputs\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Get a dataset for the initial data\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "# Define a PipelineData object to pass data between steps\n",
    "data_store = ws.get_default_datastore()\n",
    "prepped_data = OutputFileDatasetConfig('prepped')\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Script arguments include PipelineData\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\n",
    "                                      '--out_folder', prepped_data])\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'train_model.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass as script argument\n",
    "                         arguments=['--training-data', prepped_data.as_input()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the scripts themselves, you can obtain a reference to the OutputFileDatasetConfig object from the \n",
    "#script argument, and use it like a local folder.\n",
    "\n",
    "# code in data_prep.py\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--raw-ds', type=str, dest='raw_dataset_id')\n",
    "parser.add_argument('--out_folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "# Get input dataset as dataframe\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# code to prep data (in this case, just select specific columns)\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d6823",
   "metadata": {},
   "source": [
    "### Reuse pipeline steps\n",
    "\n",
    "By default, the step output from a previous pipeline run is reused without rerunning the step provided the script, source directory, and other parameters for the step have not changed. Step reuse can reduce the time it takes to run a pipeline, but it can lead to stale results when changes to downstream data sources have not been accounted for.\n",
    "\n",
    "To control reuse for an individual step, you can set the allow_reuse parameter in the step configuration, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         outputs=[prepped_data],\n",
    "                         arguments = ['--folder', prepped_data]),\n",
    "                         # Disable step reuse\n",
    "                         allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1551ac3",
   "metadata": {},
   "source": [
    "### Forcing all steps to run\n",
    "\n",
    "When you have multiple steps, you can force all of them to run regardless of individual reuse configuration by setting the regenerate_outputs parameter when submitting the pipeline experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df81f1b",
   "metadata": {},
   "source": [
    "### Publish pipelines\n",
    "\n",
    "To publish a pipeline, you can call its publish method, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088897bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, you can call the publish method on a successful run of the pipeline:\n",
    "\n",
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd215c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Published pipeline can view it in Azure Machine Learning studio\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce31e5",
   "metadata": {},
   "source": [
    "### Using a published pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ae62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#he following Python code makes a REST request to run a pipeline and displays the returned run ID.\n",
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d3d37",
   "metadata": {},
   "source": [
    "### Use pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you could use the following code to include a parameter for a \n",
    "#regularization rate in the script used by an estimator:\n",
    "\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
    "\n",
    "...\n",
    "\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass parameter as script argument\n",
    "                         arguments=['--in_folder', prepped_data,\n",
    "                                    '--reg', reg_param],\n",
    "                         inputs=[prepped_data])\n",
    "\n",
    "#You must define parameters for a pipeline before publishing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49633ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running a pipeline with a parameter\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\",\n",
    "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4960c54",
   "metadata": {},
   "source": [
    "### Schedule pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To schedule a pipeline to run at periodic intervals, you must define a ScheduleRecurrence that \n",
    "#determines the run frequency, and use it to create a Schedule.\n",
    "\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                                        description='trains model every day',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Training_Pipeline',\n",
    "                                        recurrence=daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5e55c",
   "metadata": {},
   "source": [
    "### Triggering a pipeline run on data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To schedule a pipeline to run whenever data changes, you must create a Schedule that monitors a specified path \n",
    "#on a datastore\n",
    "\n",
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline_id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da139",
   "metadata": {},
   "source": [
    "### Deploy a model as a real-time service\n",
    "\n",
    "Deployment to a local service, a compute instance, or an ACI is a good choice for testing and development. For production, you should deploy to a target that meets the specific performance, scalability, and security needs of your application architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Register a trained model\n",
    "\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=ws,\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='A classification model')\n",
    "\n",
    "#alternate way model register using run \n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6faeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define an inference configuration\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Add the dependencies for your model\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the script and environment in an InferenceConfig\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                              source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              conda_file=\"env.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a deployment configuration\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546e9b7",
   "metadata": {},
   "source": [
    "With the compute target created, you can now define the deployment configuration, which sets the target-specific compute specification for the containerized deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d992723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec26a1",
   "metadata": {},
   "source": [
    "The code to configure an ACI deployment is similar, except that you do not need to explicitly create an ACI compute target, and you must use the deploy_configuration class from the azureml.core.webservice.AciWebservice namespace. Similarly, you can use the azureml.core.webservice.LocalWebservice namespace to configure a local Docker-based service.\n",
    "\n",
    "Note:- To deploy a model to an Azure Function, you do not need to create a deployment configuration. Instead, you need to package the model based on the type of function trigger you want to use. This functionality is in preview at the time of writing. For more details, see \n",
    "Deploy a machine learning model to Azure Functions\n",
    " in the Azure Machine Learning documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829891d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Deploy the model\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88155975",
   "metadata": {},
   "source": [
    "### Consume a real-time inferencing service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Azure Machine Learning SDK\n",
    "\n",
    "#For testing, you can use the Azure Machine Learning SDK to call a web service through \n",
    "#the run method of a WebService object that references the deployed service. Typically, you send data to \n",
    "#the run method in JSON format with the following structure:\n",
    "    \n",
    "{\n",
    "  \"data\":[\n",
    "      [0.1,2.3,4.1,2.0], // 1st case\n",
    "      [0.2,1.8,3.9,2.1],  // 2nd case,\n",
    "      ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "#The response from the run method is a JSON collection with a prediction for each case that was submitted in the data. The following code sample calls a service and displays the response:\n",
    "\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "response = service.run(input_data = json_data)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(response)\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a REST endpoint \n",
    "\n",
    "#n production, most client applications will not include the Azure Machine Learning SDK, and will consume the service through its REST interface. You can determine the endpoint of a deployed service in Azure Machine Learning studio, or by retrieving the scoring_uri property of the Webservice object in the SDK, like this:\n",
    "\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316271a",
   "metadata": {},
   "source": [
    "With the endpoint known, you can use an HTTP POST request with JSON data to call the service. The following example shows how to do this using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61debf",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "In production, you will likely want to restrict access to your services by applying authentication. There are two kinds of authentication you can use:\n",
    "\n",
    "Key: Requests are authenticated by specifying the key associated with the service.\n",
    "Token: Requests are authenticated by providing a JSON Web Token (JWT).\n",
    "By default, authentication is disabled for ACI services, and set to key-based authentication for AKS services (for which primary and secondary keys are automatically generated). You can optionally configure an AKS service to use token-based authentication (which is not supported for ACI services).\n",
    "\n",
    "Assuming you have an authenticated session established with the workspace, you can retrieve the keys for a service by using the get_keys method of the WebService object associated with the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key, secondary_key = service.get_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17abf02",
   "metadata": {},
   "source": [
    "For token-based authentication, your client application needs to use service-principal authentication to verify its identity through Azure Active Directory (Azure AD) and call the get_token method of the service to retrieve a time-limited token.\n",
    "\n",
    "To make an authenticated call to the service's REST endpoint, you must include the key or token in the request header like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30527f",
   "metadata": {},
   "source": [
    "### Deploy your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77971518",
   "metadata": {},
   "source": [
    "#### Create an endpoint\n",
    "To create an online endpoint, you'll use the ManagedOnlineEndpoint class, which requires the following parameters:\n",
    "name: Name of the endpoint. Must be unique in the Azure region.\n",
    "auth_mode: Use key for key-based authentication. Use aml_token for Azure Machine Learning token-based authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3615834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=\"endpoint-example\",\n",
    "    description=\"Online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead1f6b",
   "metadata": {},
   "source": [
    "#### Deploy your MLflow model to a managed online endpoint\n",
    "When you deploy an MLflow model to a managed online endpoint, you donÂ´t need to have the scoring script and environment.\n",
    "To deploy an MLflow model, you must have model files stored on a local path or with a registered model. You can log model files when training a model by using MLflow tracking.\n",
    "In this example, we're taking the model files from a local path. The files are all stored in a local folder called model. The folder must include the MLmodel file, which describes how the model can be loaded and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ae1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# create a blue deployment\n",
    "model = Model(\n",
    "    path=\"./model\",\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    description=\"my sample mlflow model\",\n",
    ")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=\"endpoint-example\",\n",
    "    model=model,\n",
    "    instance_type=\"Standard_F4s_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1120c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To route traffic to a specific deployment,\n",
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98212463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To delete the endpoint and all associated deployments,\n",
    "ml_client.online_endpoints.begin_delete(name=\"endpoint-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71222ec",
   "metadata": {},
   "source": [
    "### Deploy a model to a managed online endpoint\n",
    "\n",
    "You can choose to deploy a model to a managed online endpoint without using the MLflow model format. To deploy a model, you'll need to create the scoring script and define the environment necessary during inferencing.\n",
    "To deploy a model, you need to have created an endpoint. Then you can deploy the model to the endpoint.\n",
    "\n",
    "#### Deploy a model to an endpoint\n",
    "To deploy a model, you must have:\n",
    "Model files stored on local path or registered model.\n",
    "A scoring script.\n",
    "An execution environment.\n",
    "The model files can be logged and stored when you train a model.\n",
    "\n",
    "#### Create the scoring script\n",
    "The scoring script needs to include two functions:\n",
    "init(): Called when the service is initialized.\n",
    "run(): Called when new data is submitted to the service.\n",
    "The init function is called when the deployment is created or updated, to load and cache the model from the model registry. The run function is called for every time the endpoint is invoked, to generate predictions from the input data. The following example Python script shows this pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ee005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# called when the deployment is created or updated\n",
    "def init():\n",
    "    global model\n",
    "    # get the path to the registered model file and load it\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# called when a request is received\n",
    "def run(raw_data):\n",
    "    # get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc84c40",
   "metadata": {},
   "source": [
    "#### Create an environment\n",
    "\n",
    "Your deployment requires an execution environment in which to run the scoring script.\n",
    "You can create an environment with a Docker image with Conda dependencies, or with a Dockerfile.\n",
    "To create an environment using a base Docker image, you can define the Conda dependencies in a conda.yml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2558dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: basic-env-cpu\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.7\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "    conda_file=\"./src/conda.yml\",\n",
    "    name=\"deployment-environment\",\n",
    "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b3946",
   "metadata": {},
   "source": [
    "#### Create the deployment\n",
    "\n",
    "When you have your model files, scoring script, and environment, you can create the deployment.\n",
    "To deploy a model to an endpoint, you can specify the compute configuration with two parameters:\n",
    "instance_type: Virtual machine (VM) size to use. Review the list of supported sizes.\n",
    "instance_count: Number of instances to use.\n",
    "To deploy the model, use the ManagedOnlineDeployment class and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3af648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment, CodeConfiguration\n",
    "\n",
    "model = Model(path=\"./model\",\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=\"endpoint-example\",\n",
    "    model=model,\n",
    "    environment=\"deployment-environment\",\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./src\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c486eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can deploy multiple models to an endpoint. To route traffic to a specific deployment,\n",
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete the endpoint and all associated deployments\n",
    "ml_client.online_endpoints.begin_delete(name=\"endpoint-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4e7c9",
   "metadata": {},
   "source": [
    "### Test managed online endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fa286",
   "metadata": {},
   "source": [
    "#### Use the Azure Machine Learning studio\n",
    "You can list all endpoints in the Azure Machine Learning studio, by navigating to the Endpoints page. In the Real-time endpoints tab, all endpoints are shown.\n",
    "You can select an endpoint to review its details and deployment logs.\n",
    "Additionally, you can use the studio to test the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d26883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"sample-data.json\",\n",
    ")\n",
    "\n",
    "if response[1]=='1':\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print (\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a8aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae12f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
